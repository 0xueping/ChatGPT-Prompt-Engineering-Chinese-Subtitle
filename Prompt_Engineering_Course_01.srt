1
00:00:00,000 --> 00:00:09,000
Welcome to this course on ChatGPT Prompt Engineering for Developers.
欢迎来到这个面向开发者的ChatGPT提示工程课程。

2
00:00:09,000 --> 00:00:14,000
I'm thrilled to have with me, Isa Fulford, to teach this along with me.
我很高兴能有Isa Fulford和我一起教授这门课。

3
00:00:14,000 --> 00:00:20,900
She is a member of the technical staff of OpenAI and had built the popular ChatGPT retrieval plugin
她是OpenAI技术团队的成员，曾经开发了备受欢迎的ChatGPT检索插件。

4
00:00:20,900 --> 00:00:27,000
and a large part of her work has been teaching people how to use LLM or large language model technology in products.
她的工作很大一部分是教人们如何在产品中使用LLM或大型语言模型技术。

5
00:00:27,000 --> 00:00:31,000
She's also contributed to the OpenAI cookbook that teaches people prompting.
她还为OpenAI食谱做出了贡献，该食谱教人如何进行提示。

6
00:00:31,000 --> 00:00:33,000
So thrilled to have you with you.

7
00:00:33,000 --> 00:00:38,000
And I'm thrilled to be here and share some prompting best practices with you all.

8
00:00:38,000 --> 00:00:45,000
So there's been a lot of material on the Internet for prompting with articles like 30 prompts everyone has to know.

9
00:00:45,000 --> 00:00:54,000
A lot of that has been focused on the ChatGPT web user interface, which many people are using to do specific and often one-off tasks.

10
00:00:54,000 --> 00:01:00,000
But I think the power of LLM's large language models as a developer too,

11
00:01:00,000 --> 00:01:04,000
that is using API calls to LLMs to quickly build software applications,

12
00:01:04,000 --> 00:01:08,000
I think that is still very underappreciated.

13
00:01:08,000 --> 00:01:12,000
In fact, my team at AIFund, which is a sister company to Deep Learning.AI,

14
00:01:12,000 --> 00:01:18,000
has been working with many startups on applying these technologies to many different applications.

15
00:01:18,000 --> 00:01:25,000
And it's been exciting to see what LLM APIs can enable developers to very quickly build.

16
00:01:25,000 --> 00:01:29,000
So in this course, we'll share with you some of the possibilities

17
00:01:29,000 --> 00:01:34,000
for what you can do as well as best practices for how you can do them.

18
00:01:34,000 --> 00:01:36,000
There's a lot of material to cover.

19
00:01:36,000 --> 00:01:41,000
First, you'll learn some prompting best practices for software development.

20
00:01:41,000 --> 00:01:47,140
Then we'll cover some common use cases, summarizing, inferring, transforming, expanding,

21
00:01:47,140 --> 00:01:50,000
and then you'll build a chatbot using an LLM.

22
00:01:50,000 --> 00:01:55,000
We hope that this will spark your imagination about new applications that you can build.

23
00:01:55,000 --> 00:01:58,000
So in the development of large language models or LLMs,

24
00:01:58,000 --> 00:02:06,000
there have been broadly two types of LLMs, which I'm going to refer to as base LLMs and instruction-tuned LLMs.

25
00:02:06,000 --> 00:02:11,000
So base LLM has been trained to predict the next word based on text training data,

26
00:02:11,000 --> 00:02:15,000
often trained on a large amount of data from the internet and other sources

27
00:02:15,000 --> 00:02:19,000
to figure out what's the next most likely word to follow.

28
00:02:19,000 --> 00:02:24,000
So for example, if you were to prompt this, once upon a time there was a unicorn,

29
00:02:24,000 --> 00:02:28,000
it may complete this, that is, it may predict the next several words are

30
00:02:28,000 --> 00:02:31,000
that live in a magical forest with all unicorn friends.

31
00:02:31,000 --> 00:02:35,000
But if you were to prompt this with what is the capital of France,

32
00:02:35,000 --> 00:02:40,000
then based on what articles on the internet might have,

33
00:02:40,000 --> 00:02:44,000
it's quite possible that the base LLM will complete this with

34
00:02:44,000 --> 00:02:48,000
what is France's largest city, what is France's population, and so on,

35
00:02:48,000 --> 00:02:52,000
because articles on the internet could quite plausibly be lists of

36
00:02:52,000 --> 00:02:55,000
quiz questions about the country of France.

37
00:02:55,000 --> 00:03:04,000
In contrast, an instruction-tuned LLM, which is where a lot of momentum of LLM research and practice has been going,

38
00:03:04,000 --> 00:03:08,000
an instruction-tuned LLM has been trained to follow instructions.

39
00:03:08,000 --> 00:03:11,000
So if you were to ask it, what is the capital of France,

40
00:03:11,000 --> 00:03:16,000
it's much more likely to output something like the capital of France is Paris.

41
00:03:16,000 --> 00:03:20,000
The way that instruction-tuned LLMs are typically trained is you start off

42
00:03:20,000 --> 00:03:24,000
with a base LLM that's been trained on a huge amount of text data

43
00:03:24,000 --> 00:03:28,000
and further train it, further fine-tune it with inputs and outputs

44
00:03:28,000 --> 00:03:32,000
that are instructions and good attempts to follow those instructions,

45
00:03:32,000 --> 00:03:36,000
and then often further refine using a technique called RLHF,

46
00:03:36,000 --> 00:03:38,500
reinforcement learning from human feedback,

47
00:03:38,500 --> 00:03:43,000
to make the system better able to be helpful and follow instructions.

48
00:03:43,000 --> 00:03:46,160
Because instruction-tuned LLMs have been trained

49
00:03:46,160 --> 00:03:48,800
to be helpful, honest, and harmless,

50
00:03:48,800 --> 00:03:54,983
so for example, they're less likely to output problematic text, such as toxic outputs, compared to base LLM,

51
00:03:55,000 --> 00:04:01,000
a lot of the practical usage scenarios have been shifting toward instruction-tuned LLMs.

52
00:04:01,000 --> 00:04:06,200
Some of the best practices you find on the internet may be more suited for a base LLM,

53
00:04:06,200 --> 00:04:08,000
but for most practical applications today,

54
00:04:08,000 --> 00:04:13,500
we would recommend most people instead focus on instruction-tuned LLMs,

55
00:04:13,500 --> 00:04:17,000
which are easier to use and also, because of the work of OpenAI/我们建议大多数人转而关注经过指导调整的LLMs。

56
00:04:17,000 --> 00:04:22,000
and other LLM companies, becoming safer and more aligned.

57
00:04:22,000 --> 00:04:27,000
So this course will focus on best practices for instruction-tuned LLMs,

58
00:04:27,000 --> 00:04:32,000
which is what we recommend you use for most of your applications.

59
00:04:32,000 --> 00:04:36,000
Before moving on, I just want to acknowledge the team from OpenAI

60
00:04:36,000 --> 00:04:39,000
and DeepLearning.ai that had contributed to the materials

61
00:04:39,000 --> 00:04:42,000
that Yizi and I will be presenting.

62
00:04:42,000 --> 00:04:45,000
I'm very grateful to Andrew Main, Joe Palermo, Boris Power,

63
00:04:45,000 --> 00:04:50,000
Ted Sanders, and Lilian Wang from OpenAI that were very involved with us

64
00:04:50,000 --> 00:04:53,000
brainstorming materials, vetting the materials to put together

65
00:04:53,000 --> 00:04:55,000
the curriculum for this short course.

66
00:04:55,000 --> 00:04:58,000
And I'm also grateful on the DeepLearning side for the work

67
00:04:58,000 --> 00:05:01,000
of Jeff Ludwig, Eddie Hsu, and Tommy Nelson.

68
00:05:01,000 --> 00:05:06,000
So when you use an instruction-tuned LLM, think of giving instructions

69
00:05:06,000 --> 00:05:10,000
to another person, say someone that's smart but doesn't know

70
00:05:10,000 --> 00:05:12,000
the specifics of your task.

71
00:05:12,000 --> 00:05:16,000
So when an LLM doesn't work, sometimes it's because the instructions

72
00:05:16,000 --> 00:05:18,000
weren't clear enough.

73
00:05:18,000 --> 00:05:21,000
For example, if you were to say, "Please write me something

74
00:05:21,000 --> 00:05:25,000
about Alan Turing," well, in addition to that, it can be helpful

75
00:05:25,000 --> 00:05:30,000
to be clear about whether you want the text to focus on his scientific work

76
00:05:30,000 --> 00:05:34,000
or his personal life or his role in history or something else.

77
00:05:34,000 --> 00:05:39,000
And if you specify what you want the tone of the text to be,

78
00:05:39,000 --> 00:05:43,000
should it take on the tone like a professional journalist would write,

79
00:05:43,000 --> 00:05:46,000
or is it more of a casual note that you dash off to a friend?

80
00:05:46,000 --> 00:05:49,000
That helps the LLM generate what you want.

81
00:05:49,000 --> 00:05:52,000
And of course, if you picture yourself asking, say,

82
00:05:52,000 --> 00:05:56,000
a fresh college graduate to carry out this task for you,

83
00:05:56,000 --> 00:05:59,000
if you can even specify what snippets of text they should read

84
00:05:59,000 --> 00:06:02,000
in advance to write this text about Alan Turing,

85
00:06:02,000 --> 00:06:06,000
then that even better sets up that fresh college grad for success

86
00:06:06,000 --> 00:06:09,000
to carry out this task for you.

87
00:06:09,000 --> 00:06:14,000
So in the next video, you see examples of how to be clear and specific,

88
00:06:14,000 --> 00:06:17,000
which is an important principle of prompting LLMs,

89
00:06:17,000 --> 00:06:21,000
and you also learn from Isa a second principle of prompting,

90
00:06:21,000 --> 00:06:24,000
that is giving the LLM time to think.

91
00:06:24,000 --> 00:06:27,000
So with that, let's go on to the next video.

92
00:06:27,000 --> 00:06:30,000
[Video played]

93
00:06:30,000 --> 00:06:40,000
[BLANK_AUDIO]

